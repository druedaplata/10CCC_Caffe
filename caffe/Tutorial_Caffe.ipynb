{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Redes Neuronales Convolucionales\n",
    "\n",
    "##Clasificación de Imágenes con Caffe y Digits.\n",
    "\n",
    "Caffe: https://github.com/BVLC/caffe\n",
    "\n",
    "Digits: https://github.com/NVIDIA/DIGITS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Arquitectura de una Red Neuronal Convolucional\n",
    "\n",
    "<img src=\"imagenes/mylenet.png\">\n",
    "\n",
    "Las redes Convolucionales son muy similares a las redes Neuronales normales. Están compuestas de neuronas que tienen pesos (weights) y bias que les permiten aprender. Cada neurona recibe unas entradas, realiza un producto punto y opcionalmente sigue una no-linealidad.}\n",
    "\n",
    "* Son redes con una estructura de conectividad especializada.\n",
    "\n",
    "\n",
    "* Entrenamiento supervisado.\n",
    "\n",
    "\n",
    "* Típicamente, tienen los siguientes elementos.\n",
    "\n",
    "  * Convolutional Layer\n",
    "  * Non-linearity\n",
    "  * Pooling Layer\n",
    "  * Fully Connected\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Convolutional Layer\n",
    "\n",
    "<img src=\"imagenes/convolution.gif\" width=400px align=center>\n",
    "\n",
    "La capa convolucional consiste en un grupo de filtros.\n",
    "Estos filtros son pequeños campos receptivos que desplazamos por todo el ancho y largo de la imagen de entrada, produciendo un mapa de características. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Conectividad Local\n",
    "\n",
    "<img src=\"imagenes/local_connected.png\" width=70%>\n",
    "\n",
    "La idea consiste en conectar cada neurona en la capa de convolución con una región pequeña en la imagen de entrada, pero conectada a todos los canales de color. <br>Esta región la llamaremos el **campo receptivo (F)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Disposición Espacial\n",
    "\n",
    "Consiste de 3 hiperparámetros que controlan la salida que obtenemos de la red convolucional.\n",
    "\n",
    "1. **Depth (Profundidad):** Controla el número de neuronas en la capa convolucional que se conectan a la misma región en la imagen de entrada. Nos referimos a un grupo de neuronas que están conectadas a la misma región como una **columna de profundidad**.\n",
    "\n",
    "2. **Stride:** Este parámetro especifica el \"paso\" que dará nuestro campo receptivo para ubicar una nueva columna de profundidad.\n",
    "\n",
    "3. **Zero-padding:** En ocasiones será conveniente definir los bordes de la entrada a la red en ceros, ya que esto nos permitirá controlar el tamaño de la salida en la red neuronal.\n",
    "\n",
    "La salida de la capa convolucional se calcula de la siguiente manera:\n",
    "\n",
    "\\begin{equation}\n",
    "  W_{output} = [(W_{input}- F + 2P)/S + 1] \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  H_{output} = [(H_{input} - F + 2P)/S + 1 ]  \n",
    "\\end{equation}\n",
    "\n",
    "Por lo tanto, si tenemos la siguiente situación:\n",
    "\n",
    "\\begin{equation}\n",
    "  Input = 227_{width} * 227_{height} * 3_{rgb}\n",
    "\\end{equation}\n",
    "\n",
    "...y definimos los hiperparámetros:\n",
    "\n",
    "\\begin{align}\n",
    "  Field = 11\\\\\n",
    "  Depth = 96\\\\\n",
    "  Stride = 4\\\\\n",
    "  Zero-padding = 0\n",
    "\\end{align}\n",
    "\n",
    "La salida de esta capa tendrá las siguientes dimensiones:\n",
    "\n",
    "\\begin{equation}\n",
    "  Output = 55_{width} * 55_{height} * 96_{neurons}\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"imagenes/conv_out_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pesos y bias compartidos\n",
    "\n",
    "Hasta el momento cada neurona en nuestra red convolucional tiene $1$ bias y $11$x$11$ pesos conectados al campo perceptivo. \n",
    "\n",
    "Puesto que las imágenes naturales tienen propiedades **estacionarias**, es decir que las características aprendidas en una parte de la imagen serán útiles en cualquier otra parte.\n",
    "\n",
    "Gracias a esto podemos usar el mismo bias y $11$x$11$ pesos en todas las neuronas de la primera capa para que busquen, por ejemplo, una linea vertical en toda la imagen. Y usaremos pesos diferentes en las demas capas de neuronas para buscar diferentes características.\n",
    "\n",
    "<img src=\"imagenes/conv_out_2.png\" >\n",
    "\n",
    "Despues del entrenamiento estos $96$ pesos de $11$x$11$ aprenderán a reconocer características simples de la imagen.\n",
    "\n",
    "<img src=\"imagenes/features_conv1.jpg\" width=250>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Non-linearity Layer\n",
    "\n",
    "Podemos definir la salida de una capa en la red neuronal como $\\bar o$ que sería el resultado\n",
    "de una función de activación $f()$ aplicada al producto entre la entrada\n",
    "\n",
    "$\\bar x=\\begin{bmatrix}x_1\\\\x_2\\\\.\\\\.\\\\x_3\\\\x_4\\end{bmatrix}$ y los pesos de la capa \n",
    "$W=\\begin{bmatrix}w_{00} & w_{01} & . & . & w_{0n} & b_0\\\\\n",
    "                  w_{10} & w_{11} & . & . & w_{1n} & b_1\\\\\n",
    "                  . & . & . & . & . & .\\\\\n",
    "                  w_{n0} & w_{n1} & . & . & w_{nn} & b_n\\\\\\end{bmatrix}$ produciendo $\\bar o = f(W\\bar x)$\n",
    "                  \n",
    "Lo que podemos observar en este caso es que si usamos activación **lineal**, entonces una red neuronal con 2 capas se describiría de la siguiente forma:\n",
    "\n",
    "$\\bar o_1 = W_1\\bar x$\n",
    "\n",
    "$\\bar o_2 = W_2\\bar o_1 = W_2W_1\\bar x$\n",
    "\n",
    "Esto significa que añadir mas capas hara el sistemas mas lento y complicado, pero no mas expresivo. Esto no se cumple cuando usamos una función de activación **no lineal**.\n",
    "\n",
    "<img src=\"imagenes/relu.png\" style=\"float: left\">\n",
    "<img src=\"imagenes/sigmoid2.png\" style=\"float: left\" width=300>\n",
    "<img src=\"imagenes/tanh2.png\" style=\"float: left\" width=300>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "##Pooling Layer\n",
    "\n",
    "La función de esta capa es reducir el tamaño de la representación entregada por la capa convolucional para disminuir la cantidad de parámetros y disminuir el tiempo de calculo en la red.\n",
    "\n",
    "\n",
    "<img src=\"imagenes/pooling.png\" width=50%>\n",
    "\n",
    "\n",
    "Al igual que la red convolucional, recibe hiperparámetros que controlan su funcionamiento, el campo receptivo $F$ y el paso $S$. Frecuentemente se usa un campo receptivo pequeño, dado que con un mayor tamaño se perdería la representación en las características de la red.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##Fully Connected Layer\n",
    "\n",
    "Finalmente, después de varias capas convolucionales, no linealidades y posiblemente pooling, el razonamiento de alto nivel es realizado por las capas fully connected. Estas toman todas las neuronas en la capa anterior (ya sea convolucional, pooling o fully connected) y las conecta con todas las neuronas que tiene. \n",
    "\n",
    "<img src=\"imagenes/fully_connected.gif\">\n",
    "\n",
    "La última capa fully connected en el caso de clasificación de imágenes tendrá un número de neuronas igual a la cantidad de clases que estamos clasificando.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Entrenar una CNN usando CAFFE\n",
    "\n",
    "1. Seleccionar un dataset.\n",
    "2. Crear una estructura de red.\n",
    "3. Definir los hiperparámetros que guiarán el entrenamiento.\n",
    "4. Entrenar la red y evaluar la precisión sobre un dataset de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##1. Seleccionar un dataset\n",
    "\n",
    "Para este ejercicio usamos el dataset **Cifar 10**, que contiene objetos y animales de 10 categorias. El dataset consiste de 60000 imágenes de de 32x32. Para este ejercicio usaremos 45000 imágenes para entrenamiento y 5000 en validación.\n",
    "\n",
    "<img src=\"imagenes/cifar_10.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###1.1 Convertir a formato LMDB\n",
    "\n",
    "* Un archivo con la ruta a todas las imagenes y la categoría a la que pertenecen.\n",
    "\n",
    "        imagen_001.jpg 0\n",
    "        imagen_002.jpg 1\n",
    "        imagen_003.jpg 1\n",
    "        imagen_004.jpg 2\n",
    "        imagen_005.jpg 2\n",
    "        imagen_006.jpg 0\n",
    "        imagen_007.jpg 3\n",
    "        imagen_008.jpg 4\n",
    "\n",
    "\n",
    "* Las imágenes deben tener el mismo tamaño.\n",
    "* Convertimos las imagenes en formato LMDB para mejorar el tiempo de acceso de CAFFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train lmdb...\n",
      "Creating val lmdb...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0724 18:26:35.286201  7757 convert_imageset.cpp:79] Shuffling data\n",
      "I0724 18:26:35.444700  7757 convert_imageset.cpp:82] A total of 45000 images.\n",
      "I0724 18:26:35.444989  7757 db.cpp:34] Opened lmdb /home/sandiego/Programas/datasets/cifar10/cifar_train_lmdb\n",
      "E0724 18:26:35.718617  7757 convert_imageset.cpp:143] Processed 1000 files.\n",
      "E0724 18:26:36.007438  7757 convert_imageset.cpp:143] Processed 2000 files.\n",
      "E0724 18:26:36.285279  7757 convert_imageset.cpp:143] Processed 3000 files.\n",
      "E0724 18:26:36.574103  7757 convert_imageset.cpp:143] Processed 4000 files.\n",
      "E0724 18:26:36.874199  7757 convert_imageset.cpp:143] Processed 5000 files.\n",
      "E0724 18:26:37.174136  7757 convert_imageset.cpp:143] Processed 6000 files.\n",
      "E0724 18:26:37.463117  7757 convert_imageset.cpp:143] Processed 7000 files.\n",
      "E0724 18:26:37.763088  7757 convert_imageset.cpp:143] Processed 8000 files.\n",
      "E0724 18:26:38.063148  7757 convert_imageset.cpp:143] Processed 9000 files.\n",
      "E0724 18:26:38.363109  7757 convert_imageset.cpp:143] Processed 10000 files.\n",
      "E0724 18:26:38.663179  7757 convert_imageset.cpp:143] Processed 11000 files.\n",
      "E0724 18:26:38.963126  7757 convert_imageset.cpp:143] Processed 12000 files.\n",
      "E0724 18:26:39.252086  7757 convert_imageset.cpp:143] Processed 13000 files.\n",
      "E0724 18:26:39.540889  7757 convert_imageset.cpp:143] Processed 14000 files.\n",
      "E0724 18:26:39.829879  7757 convert_imageset.cpp:143] Processed 15000 files.\n",
      "E0724 18:26:40.140955  7757 convert_imageset.cpp:143] Processed 16000 files.\n",
      "E0724 18:26:40.452132  7757 convert_imageset.cpp:143] Processed 17000 files.\n",
      "E0724 18:26:40.740984  7757 convert_imageset.cpp:143] Processed 18000 files.\n",
      "E0724 18:26:41.041059  7757 convert_imageset.cpp:143] Processed 19000 files.\n",
      "E0724 18:26:41.352115  7757 convert_imageset.cpp:143] Processed 20000 files.\n",
      "E0724 18:26:41.682333  7757 convert_imageset.cpp:143] Processed 21000 files.\n",
      "E0724 18:26:41.996582  7757 convert_imageset.cpp:143] Processed 22000 files.\n",
      "E0724 18:26:42.296658  7757 convert_imageset.cpp:143] Processed 23000 files.\n",
      "E0724 18:26:42.585464  7757 convert_imageset.cpp:143] Processed 24000 files.\n",
      "E0724 18:26:42.896689  7757 convert_imageset.cpp:143] Processed 25000 files.\n",
      "E0724 18:26:43.185515  7757 convert_imageset.cpp:143] Processed 26000 files.\n",
      "E0724 18:26:43.485591  7757 convert_imageset.cpp:143] Processed 27000 files.\n",
      "E0724 18:26:44.152220  7757 convert_imageset.cpp:143] Processed 28000 files.\n",
      "E0724 18:26:44.441181  7757 convert_imageset.cpp:143] Processed 29000 files.\n",
      "E0724 18:26:44.718911  7757 convert_imageset.cpp:143] Processed 30000 files.\n",
      "E0724 18:26:45.019019  7757 convert_imageset.cpp:143] Processed 31000 files.\n",
      "E0724 18:26:45.352284  7757 convert_imageset.cpp:143] Processed 32000 files.\n",
      "E0724 18:26:45.641234  7757 convert_imageset.cpp:143] Processed 33000 files.\n",
      "E0724 18:26:45.963404  7757 convert_imageset.cpp:143] Processed 34000 files.\n",
      "E0724 18:26:46.263487  7757 convert_imageset.cpp:143] Processed 35000 files.\n",
      "E0724 18:26:46.563424  7757 convert_imageset.cpp:143] Processed 36000 files.\n",
      "E0724 18:26:46.863530  7757 convert_imageset.cpp:143] Processed 37000 files.\n",
      "E0724 18:26:47.163470  7757 convert_imageset.cpp:143] Processed 38000 files.\n",
      "E0724 18:26:47.474661  7757 convert_imageset.cpp:143] Processed 39000 files.\n",
      "E0724 18:26:47.785715  7757 convert_imageset.cpp:143] Processed 40000 files.\n",
      "E0724 18:26:48.096876  7757 convert_imageset.cpp:143] Processed 41000 files.\n",
      "E0724 18:26:48.430174  7757 convert_imageset.cpp:143] Processed 42000 files.\n",
      "E0724 18:26:48.752470  7757 convert_imageset.cpp:143] Processed 43000 files.\n",
      "E0724 18:26:49.063539  7757 convert_imageset.cpp:143] Processed 44000 files.\n",
      "E0724 18:26:49.363598  7757 convert_imageset.cpp:143] Processed 45000 files.\n",
      "I0724 18:26:49.628402  7761 convert_imageset.cpp:79] Shuffling data\n",
      "I0724 18:26:49.784503  7761 convert_imageset.cpp:82] A total of 5000 images.\n",
      "I0724 18:26:49.784770  7761 db.cpp:34] Opened lmdb /home/sandiego/Programas/datasets/cifar10/cifar_val_lmdb\n",
      "E0724 18:26:49.996245  7761 convert_imageset.cpp:143] Processed 1000 files.\n",
      "E0724 18:26:50.185055  7761 convert_imageset.cpp:143] Processed 2000 files.\n",
      "E0724 18:26:50.374022  7761 convert_imageset.cpp:143] Processed 3000 files.\n",
      "E0724 18:26:50.585073  7761 convert_imageset.cpp:143] Processed 4000 files.\n",
      "E0724 18:26:50.785150  7761 convert_imageset.cpp:143] Processed 5000 files.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env sh\n",
    "\n",
    "# Ruta donde se guardaran los archivos LMDB\n",
    "OUTPUT=/home/sandiego/Programas/datasets/cifar10\n",
    "\n",
    "# Ruta donde se encuentran los datos a transformar\n",
    "DATA=/home/sandiego/Programas/datasets/cifar10\n",
    "\n",
    "# Ruta de la carpeta TOOLS en la instalación de Caffe\n",
    "TOOLS=/home/sandiego/Programas/caffe/build/tools\n",
    "\n",
    "TRAIN_DATA_ROOT=$DATA/train/\n",
    "VAL_DATA_ROOT=$DATA/val/\n",
    "\n",
    "echo \"Creating train lmdb...\"\n",
    "\n",
    "GLOG_logtostderr=1 $TOOLS/convert_imageset \\\n",
    "    --shuffle \\\n",
    "    $TRAIN_DATA_ROOT \\\n",
    "    $DATA/train/train.txt \\\n",
    "    $OUTPUT/cifar_train_lmdb\n",
    "\n",
    "echo \"Creating val lmdb...\"\n",
    "\n",
    "GLOG_logtostderr=1 $TOOLS/convert_imageset \\\n",
    "    --shuffle \\\n",
    "    $VAL_DATA_ROOT \\\n",
    "    $DATA/val/val.txt \\\n",
    "    $OUTPUT/cifar_val_lmdb\n",
    "\n",
    "echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##2. Elegir una estructura de red\n",
    "\n",
    "<img src=\"imagenes/mylenet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.1 Data Layer \n",
    "\n",
    "``` python\n",
    "                layer {\n",
    "                  name: \"data\"\n",
    "                  type: \"Data\"\n",
    "                  top: \"data\"\n",
    "                  top: \"label\"\n",
    "                  include {\n",
    "                    phase: TRAIN\n",
    "                  }\n",
    "                  transform_param {\n",
    "                    # mean_file: \"mean.binaryproto\"\n",
    "                    # mirror: true\n",
    "                    # crop_size: 28\n",
    "                    # scale: 0,00390625\n",
    "                  }\n",
    "                  data_param {\n",
    "                    source: \"cifar_train_lmdb\"\n",
    "                    batch_size: 100\n",
    "                    backend: LMDB\n",
    "                  }\n",
    "                }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.2 Convolution Layer\n",
    "\n",
    "<img src=\"imagenes/convolution_text.png\" align=left>\n",
    "<img src=\"imagenes/convolution.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.3 Rectified Linear Layer\n",
    "\n",
    "<img src=\"imagenes/relu_text.png\" align=left>\n",
    "<img src=\"imagenes/relu.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.4 Pooling Layer\n",
    "\n",
    "\n",
    "<img src=\"imagenes/pooling_text.png\" align=left>\n",
    "<img src=\"imagenes/pooling.gif\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.5 Fully Connected Layer\n",
    "\n",
    "<img src=\"imagenes/fully_connected_text.png\" align=left>\n",
    "<img src=\"imagenes/fully_connected.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##3. Definir un solver para guiar el entrenamiento\n",
    "\n",
    "\n",
    "``` python\n",
    "\n",
    "# Con este cubrimos todas las imagenes de prueba -> batch_size * test_iter > test images\n",
    "test_iter: 10\n",
    "# A qué intervalos realizaremos las pruebas\n",
    "test_interval: 88 \n",
    "# A qué intervalo mostrar el valor de la función de costo\n",
    "display: 100\n",
    "# Número máximo de iteraciones\n",
    "max_iter: 4000 \n",
    "# Tasa de aprendizaje de la red\n",
    "base_lr: 0.01 \n",
    "# Política de Aprendizaje\n",
    "lr_policy: \"step\"\n",
    "gamma: 0.1\n",
    "momentum: 0.9\n",
    "weight_decay: 0.0005\n",
    "stepsize: 500\n",
    "# Intervalo para guardar resultados intermedios\n",
    "snapshot: 2000\n",
    "snapshot_prefix: \"snapshot\"\n",
    "# Red que usaremos\n",
    "net: \"train_val.prototxt\"\n",
    "# Modo de trabajo\n",
    "solver_mode: GPU # CPU o GPU\n",
    "solver_type: SGD # SGD, NAG y Adagrad\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##4. Entrenar la red y evaluar la precisión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0730 14:32:59.289124  5916 caffe.cpp:155] Using GPUs 0\n",
      "I0730 14:32:59.451802  5916 solver.cpp:33] Initializing solver from parameters: \n",
      "test_iter: 10\n",
      "test_interval: 88\n",
      "base_lr: 0.0001\n",
      "display: 100\n",
      "max_iter: 2640\n",
      "lr_policy: \"step\"\n",
      "gamma: 0.1\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "stepsize: 872\n",
      "snapshot: 1320\n",
      "snapshot_prefix: \"snapshot\"\n",
      "solver_mode: GPU\n",
      "device_id: 0\n",
      "random_seed: 1989\n",
      "net: \"/home/sandiego/septiembre/caffe/train_caffe/train_val.prototxt\"\n",
      "solver_type: SGD\n",
      "I0730 14:32:59.451900  5916 solver.cpp:81] Creating training net from net file: /home/sandiego/septiembre/caffe/train_caffe/train_val.prototxt\n",
      "I0730 14:32:59.452219  5916 net.cpp:316] The NetState phase (0) differed from the phase (1) specified by a rule in layer data\n",
      "I0730 14:32:59.452236  5916 net.cpp:316] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0730 14:32:59.452312  5916 net.cpp:47] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"/home/sandiego/Programas/datasets/cifar10/cifar_train_lmdb\"\n",
      "    batch_size: 512\n",
      "    backend: LMDB\n",
      "    prefetch: 4\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 64\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"fc8\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0730 14:32:59.452563  5916 layer_factory.hpp:75] Creating layer data\n",
      "I0730 14:32:59.452970  5916 net.cpp:99] Creating Layer data\n",
      "I0730 14:32:59.453008  5916 net.cpp:409] data -> data\n",
      "I0730 14:32:59.453047  5916 net.cpp:409] data -> label\n",
      "I0730 14:32:59.453059  5916 net.cpp:131] Setting up data\n",
      "I0730 14:32:59.453989  5920 db.cpp:34] Opened lmdb /home/sandiego/Programas/datasets/cifar10/cifar_train_lmdb\n",
      "I0730 14:32:59.454063  5916 data_layer.cpp:60] output data size: 512,3,32,32\n",
      "I0730 14:32:59.465737  5916 net.cpp:140] Top shape: 512 3 32 32 (1572864)\n",
      "I0730 14:32:59.465780  5916 net.cpp:140] Top shape: 512 (512)\n",
      "I0730 14:32:59.465791  5916 layer_factory.hpp:75] Creating layer conv1\n",
      "I0730 14:32:59.465813  5916 net.cpp:99] Creating Layer conv1\n",
      "I0730 14:32:59.465821  5916 net.cpp:453] conv1 <- data\n",
      "I0730 14:32:59.465837  5916 net.cpp:409] conv1 -> conv1\n",
      "I0730 14:32:59.465862  5916 net.cpp:131] Setting up conv1\n",
      "I0730 14:32:59.512254  5916 net.cpp:140] Top shape: 512 64 32 32 (33554432)\n",
      "I0730 14:32:59.512351  5916 layer_factory.hpp:75] Creating layer relu1\n",
      "I0730 14:32:59.512382  5916 net.cpp:99] Creating Layer relu1\n",
      "I0730 14:32:59.512394  5916 net.cpp:453] relu1 <- conv1\n",
      "I0730 14:32:59.512408  5916 net.cpp:396] relu1 -> conv1 (in-place)\n",
      "I0730 14:32:59.512423  5916 net.cpp:131] Setting up relu1\n",
      "I0730 14:32:59.512688  5916 net.cpp:140] Top shape: 512 64 32 32 (33554432)\n",
      "I0730 14:32:59.512701  5916 layer_factory.hpp:75] Creating layer pool1\n",
      "I0730 14:32:59.512713  5916 net.cpp:99] Creating Layer pool1\n",
      "I0730 14:32:59.512720  5916 net.cpp:453] pool1 <- conv1\n",
      "I0730 14:32:59.512729  5916 net.cpp:409] pool1 -> pool1\n",
      "I0730 14:32:59.512737  5916 net.cpp:131] Setting up pool1\n",
      "I0730 14:32:59.512809  5916 net.cpp:140] Top shape: 512 64 16 16 (8388608)\n",
      "I0730 14:32:59.512819  5916 layer_factory.hpp:75] Creating layer fc8\n",
      "I0730 14:32:59.512841  5916 net.cpp:99] Creating Layer fc8\n",
      "I0730 14:32:59.512868  5916 net.cpp:453] fc8 <- pool1\n",
      "I0730 14:32:59.512877  5916 net.cpp:409] fc8 -> fc8\n",
      "I0730 14:32:59.512889  5916 net.cpp:131] Setting up fc8\n",
      "I0730 14:32:59.518395  5916 net.cpp:140] Top shape: 512 10 (5120)\n",
      "I0730 14:32:59.518467  5916 layer_factory.hpp:75] Creating layer loss\n",
      "I0730 14:32:59.518514  5916 net.cpp:99] Creating Layer loss\n",
      "I0730 14:32:59.518532  5916 net.cpp:453] loss <- fc8\n",
      "I0730 14:32:59.518551  5916 net.cpp:453] loss <- label\n",
      "I0730 14:32:59.518571  5916 net.cpp:409] loss -> loss\n",
      "I0730 14:32:59.518595  5916 net.cpp:131] Setting up loss\n",
      "I0730 14:32:59.518620  5916 layer_factory.hpp:75] Creating layer loss\n",
      "I0730 14:32:59.518919  5916 net.cpp:140] Top shape: (1)\n",
      "I0730 14:32:59.518951  5916 net.cpp:145]     with loss weight 1\n",
      "I0730 14:32:59.519006  5916 net.cpp:213] loss needs backward computation.\n",
      "I0730 14:32:59.519023  5916 net.cpp:213] fc8 needs backward computation.\n",
      "I0730 14:32:59.519042  5916 net.cpp:213] pool1 needs backward computation.\n",
      "I0730 14:32:59.519059  5916 net.cpp:213] relu1 needs backward computation.\n",
      "I0730 14:32:59.519076  5916 net.cpp:213] conv1 needs backward computation.\n",
      "I0730 14:32:59.519090  5916 net.cpp:217] data does not need backward computation.\n",
      "I0730 14:32:59.519099  5916 net.cpp:260] This network produces output loss\n",
      "I0730 14:32:59.519114  5916 net.cpp:529] Collecting Learning Rate and Weight Decay.\n",
      "I0730 14:32:59.519129  5916 net.cpp:274] Network initialization done.\n",
      "I0730 14:32:59.519139  5916 net.cpp:275] Memory required for data: 308303876\n",
      "I0730 14:32:59.519551  5916 solver.cpp:167] Creating test net (#0) specified by net file: /home/sandiego/septiembre/caffe/train_caffe/train_val.prototxt\n",
      "I0730 14:32:59.519600  5916 net.cpp:316] The NetState phase (1) differed from the phase (0) specified by a rule in layer data\n",
      "I0730 14:32:59.519718  5916 net.cpp:47] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"data\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"/home/sandiego/Programas/datasets/cifar10/cifar_val_lmdb\"\n",
      "    batch_size: 512\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 64\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"fc8\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"fc8\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "I0730 14:32:59.520150  5916 layer_factory.hpp:75] Creating layer data\n",
      "I0730 14:32:59.520520  5916 net.cpp:99] Creating Layer data\n",
      "I0730 14:32:59.520535  5916 net.cpp:409] data -> data\n",
      "I0730 14:32:59.520550  5916 net.cpp:409] data -> label\n",
      "I0730 14:32:59.520558  5916 net.cpp:131] Setting up data\n",
      "I0730 14:32:59.521416  5922 db.cpp:34] Opened lmdb /home/sandiego/Programas/datasets/cifar10/cifar_val_lmdb\n",
      "I0730 14:32:59.521461  5916 data_layer.cpp:60] output data size: 512,3,32,32\n",
      "I0730 14:32:59.531975  5916 net.cpp:140] Top shape: 512 3 32 32 (1572864)\n",
      "I0730 14:32:59.532011  5916 net.cpp:140] Top shape: 512 (512)\n",
      "I0730 14:32:59.532023  5916 layer_factory.hpp:75] Creating layer label_data_1_split\n",
      "I0730 14:32:59.532047  5916 net.cpp:99] Creating Layer label_data_1_split\n",
      "I0730 14:32:59.532079  5916 net.cpp:453] label_data_1_split <- label\n",
      "I0730 14:32:59.532094  5916 net.cpp:409] label_data_1_split -> label_data_1_split_0\n",
      "I0730 14:32:59.532110  5916 net.cpp:409] label_data_1_split -> label_data_1_split_1\n",
      "I0730 14:32:59.532122  5916 net.cpp:131] Setting up label_data_1_split\n",
      "I0730 14:32:59.532138  5916 net.cpp:140] Top shape: 512 (512)\n",
      "I0730 14:32:59.532148  5916 net.cpp:140] Top shape: 512 (512)\n",
      "I0730 14:32:59.532157  5916 layer_factory.hpp:75] Creating layer conv1\n",
      "I0730 14:32:59.532172  5916 net.cpp:99] Creating Layer conv1\n",
      "I0730 14:32:59.532181  5916 net.cpp:453] conv1 <- data\n",
      "I0730 14:32:59.532192  5916 net.cpp:409] conv1 -> conv1\n",
      "I0730 14:32:59.532207  5916 net.cpp:131] Setting up conv1\n",
      "I0730 14:32:59.533033  5916 net.cpp:140] Top shape: 512 64 32 32 (33554432)\n",
      "I0730 14:32:59.533061  5916 layer_factory.hpp:75] Creating layer relu1\n",
      "I0730 14:32:59.533077  5916 net.cpp:99] Creating Layer relu1\n",
      "I0730 14:32:59.533085  5916 net.cpp:453] relu1 <- conv1\n",
      "I0730 14:32:59.533097  5916 net.cpp:396] relu1 -> conv1 (in-place)\n",
      "I0730 14:32:59.533107  5916 net.cpp:131] Setting up relu1\n",
      "I0730 14:32:59.533360  5916 net.cpp:140] Top shape: 512 64 32 32 (33554432)\n",
      "I0730 14:32:59.533377  5916 layer_factory.hpp:75] Creating layer pool1\n",
      "I0730 14:32:59.533392  5916 net.cpp:99] Creating Layer pool1\n",
      "I0730 14:32:59.533401  5916 net.cpp:453] pool1 <- conv1\n",
      "I0730 14:32:59.533412  5916 net.cpp:409] pool1 -> pool1\n",
      "I0730 14:32:59.533424  5916 net.cpp:131] Setting up pool1\n",
      "I0730 14:32:59.533499  5916 net.cpp:140] Top shape: 512 64 16 16 (8388608)\n",
      "I0730 14:32:59.533510  5916 layer_factory.hpp:75] Creating layer fc8\n",
      "I0730 14:32:59.533524  5916 net.cpp:99] Creating Layer fc8\n",
      "I0730 14:32:59.533532  5916 net.cpp:453] fc8 <- pool1\n",
      "I0730 14:32:59.533543  5916 net.cpp:409] fc8 -> fc8\n",
      "I0730 14:32:59.533556  5916 net.cpp:131] Setting up fc8\n",
      "I0730 14:32:59.540277  5916 net.cpp:140] Top shape: 512 10 (5120)\n",
      "I0730 14:32:59.540314  5916 layer_factory.hpp:75] Creating layer fc8_fc8_0_split\n",
      "I0730 14:32:59.540330  5916 net.cpp:99] Creating Layer fc8_fc8_0_split\n",
      "I0730 14:32:59.540340  5916 net.cpp:453] fc8_fc8_0_split <- fc8\n",
      "I0730 14:32:59.540354  5916 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_0\n",
      "I0730 14:32:59.540367  5916 net.cpp:409] fc8_fc8_0_split -> fc8_fc8_0_split_1\n",
      "I0730 14:32:59.540380  5916 net.cpp:131] Setting up fc8_fc8_0_split\n",
      "I0730 14:32:59.540392  5916 net.cpp:140] Top shape: 512 10 (5120)\n",
      "I0730 14:32:59.540403  5916 net.cpp:140] Top shape: 512 10 (5120)\n",
      "I0730 14:32:59.540413  5916 layer_factory.hpp:75] Creating layer loss\n",
      "I0730 14:32:59.540424  5916 net.cpp:99] Creating Layer loss\n",
      "I0730 14:32:59.540432  5916 net.cpp:453] loss <- fc8_fc8_0_split_0\n",
      "I0730 14:32:59.540452  5916 net.cpp:453] loss <- label_data_1_split_0\n",
      "I0730 14:32:59.540462  5916 net.cpp:409] loss -> loss\n",
      "I0730 14:32:59.540487  5916 net.cpp:131] Setting up loss\n",
      "I0730 14:32:59.540498  5916 layer_factory.hpp:75] Creating layer loss\n",
      "I0730 14:32:59.540655  5916 net.cpp:140] Top shape: (1)\n",
      "I0730 14:32:59.540678  5916 net.cpp:145]     with loss weight 1\n",
      "I0730 14:32:59.540704  5916 layer_factory.hpp:75] Creating layer accuracy\n",
      "I0730 14:32:59.540724  5916 net.cpp:99] Creating Layer accuracy\n",
      "I0730 14:32:59.540734  5916 net.cpp:453] accuracy <- fc8_fc8_0_split_1\n",
      "I0730 14:32:59.540757  5916 net.cpp:453] accuracy <- label_data_1_split_1\n",
      "I0730 14:32:59.540768  5916 net.cpp:409] accuracy -> accuracy\n",
      "I0730 14:32:59.540791  5916 net.cpp:131] Setting up accuracy\n",
      "I0730 14:32:59.540805  5916 net.cpp:140] Top shape: (1)\n",
      "I0730 14:32:59.540814  5916 net.cpp:217] accuracy does not need backward computation.\n",
      "I0730 14:32:59.540824  5916 net.cpp:213] loss needs backward computation.\n",
      "I0730 14:32:59.540833  5916 net.cpp:213] fc8_fc8_0_split needs backward computation.\n",
      "I0730 14:32:59.540843  5916 net.cpp:213] fc8 needs backward computation.\n",
      "I0730 14:32:59.540851  5916 net.cpp:213] pool1 needs backward computation.\n",
      "I0730 14:32:59.540868  5916 net.cpp:213] relu1 needs backward computation.\n",
      "I0730 14:32:59.540877  5916 net.cpp:213] conv1 needs backward computation.\n",
      "I0730 14:32:59.540909  5916 net.cpp:217] label_data_1_split does not need backward computation.\n",
      "I0730 14:32:59.540920  5916 net.cpp:217] data does not need backward computation.\n",
      "I0730 14:32:59.540930  5916 net.cpp:260] This network produces output accuracy\n",
      "I0730 14:32:59.540946  5916 net.cpp:260] This network produces output loss\n",
      "I0730 14:32:59.540966  5916 net.cpp:529] Collecting Learning Rate and Weight Decay.\n",
      "I0730 14:32:59.540976  5916 net.cpp:274] Network initialization done.\n",
      "I0730 14:32:59.540983  5916 net.cpp:275] Memory required for data: 308348936\n",
      "I0730 14:32:59.541040  5916 solver.cpp:45] Solver scaffolding done.\n",
      "I0730 14:32:59.541074  5916 caffe.cpp:175] Starting Optimization\n",
      "I0730 14:32:59.541090  5916 solver.cpp:269] Solving \n",
      "I0730 14:32:59.541102  5916 solver.cpp:270] Learning Rate Policy: step\n",
      "I0730 14:32:59.541136  5916 solver.cpp:314] Iteration 0, Testing net (#0)\n",
      "I0730 14:32:59.541560  5916 blocking_queue.cpp:50] Data layer prefetch queue empty\n",
      "I0730 14:32:59.864437  5916 solver.cpp:363]     Test net output #0: accuracy = 0.110352\n",
      "I0730 14:32:59.864480  5916 solver.cpp:363]     Test net output #1: loss = 14.4475 (* 1 = 14.4475 loss)\n",
      "I0730 14:32:59.898566  5916 solver.cpp:217] Iteration 0, loss = 15.1049\n",
      "I0730 14:32:59.898612  5916 solver.cpp:234]     Train net output #0: loss = 15.1049 (* 1 = 15.1049 loss)\n",
      "I0730 14:32:59.950891  5916 solver.cpp:511] Iteration 0 (0/s), lr = 0.0001\n",
      "I0730 14:33:06.843205  5916 solver.cpp:314] Iteration 88, Testing net (#0)\n",
      "I0730 14:33:07.142359  5916 solver.cpp:363]     Test net output #0: accuracy = 0.200586\n",
      "I0730 14:33:07.142395  5916 solver.cpp:363]     Test net output #1: loss = 2.1739 (* 1 = 2.1739 loss)\n",
      "I0730 14:33:08.123122  5916 solver.cpp:217] Iteration 100, loss = 2.15307\n",
      "I0730 14:33:08.123169  5916 solver.cpp:234]     Train net output #0: loss = 2.15307 (* 1 = 2.15307 loss)\n",
      "I0730 14:33:08.171774  5916 solver.cpp:511] Iteration 100 (12.1642/s), lr = 0.0001\n",
      "I0730 14:33:14.114651  5916 solver.cpp:314] Iteration 176, Testing net (#0)\n",
      "I0730 14:33:14.414736  5916 solver.cpp:363]     Test net output #0: accuracy = 0.292578\n",
      "I0730 14:33:14.414774  5916 solver.cpp:363]     Test net output #1: loss = 1.9788 (* 1 = 1.9788 loss)\n",
      "I0730 14:33:16.345578  5916 solver.cpp:217] Iteration 200, loss = 1.92053\n",
      "I0730 14:33:16.345628  5916 solver.cpp:234]     Train net output #0: loss = 1.92053 (* 1 = 1.92053 loss)\n",
      "I0730 14:33:16.394471  5916 solver.cpp:511] Iteration 200 (12.1616/s), lr = 0.0001\n",
      "I0730 14:33:21.384316  5916 solver.cpp:314] Iteration 264, Testing net (#0)\n",
      "I0730 14:33:21.684248  5916 solver.cpp:363]     Test net output #0: accuracy = 0.318555\n",
      "I0730 14:33:21.684344  5916 solver.cpp:363]     Test net output #1: loss = 1.9193 (* 1 = 1.9193 loss)\n",
      "I0730 14:33:24.568078  5916 solver.cpp:217] Iteration 300, loss = 1.80437\n",
      "I0730 14:33:24.568114  5916 solver.cpp:234]     Train net output #0: loss = 1.80437 (* 1 = 1.80437 loss)\n",
      "I0730 14:33:24.617008  5916 solver.cpp:511] Iteration 300 (12.1618/s), lr = 0.0001\n",
      "I0730 14:33:28.661205  5916 solver.cpp:314] Iteration 352, Testing net (#0)\n",
      "I0730 14:33:28.960501  5916 solver.cpp:363]     Test net output #0: accuracy = 0.348633\n",
      "I0730 14:33:28.960538  5916 solver.cpp:363]     Test net output #1: loss = 1.85511 (* 1 = 1.85511 loss)\n",
      "I0730 14:33:32.795595  5916 solver.cpp:217] Iteration 400, loss = 1.82877\n",
      "I0730 14:33:32.795723  5916 solver.cpp:234]     Train net output #0: loss = 1.82877 (* 1 = 1.82877 loss)\n",
      "I0730 14:33:32.844243  5916 solver.cpp:511] Iteration 400 (12.1549/s), lr = 0.0001\n",
      "I0730 14:33:35.940026  5916 solver.cpp:314] Iteration 440, Testing net (#0)\n",
      "I0730 14:33:36.240375  5916 solver.cpp:363]     Test net output #0: accuracy = 0.367969\n",
      "I0730 14:33:36.240420  5916 solver.cpp:363]     Test net output #1: loss = 1.78534 (* 1 = 1.78534 loss)\n",
      "I0730 14:33:41.025193  5916 solver.cpp:217] Iteration 500, loss = 1.87759\n",
      "I0730 14:33:41.025233  5916 solver.cpp:234]     Train net output #0: loss = 1.87759 (* 1 = 1.87759 loss)\n",
      "I0730 14:33:41.074148  5916 solver.cpp:511] Iteration 500 (12.1509/s), lr = 0.0001\n",
      "I0730 14:33:43.216632  5916 solver.cpp:314] Iteration 528, Testing net (#0)\n",
      "I0730 14:33:43.516116  5916 solver.cpp:363]     Test net output #0: accuracy = 0.380469\n",
      "I0730 14:33:43.516158  5916 solver.cpp:363]     Test net output #1: loss = 1.7476 (* 1 = 1.7476 loss)\n",
      "I0730 14:33:49.253808  5916 solver.cpp:217] Iteration 600, loss = 1.69635\n",
      "I0730 14:33:49.253854  5916 solver.cpp:234]     Train net output #0: loss = 1.69635 (* 1 = 1.69635 loss)\n",
      "I0730 14:33:49.302449  5916 solver.cpp:511] Iteration 600 (12.1533/s), lr = 0.0001\n",
      "I0730 14:33:50.490862  5916 solver.cpp:314] Iteration 616, Testing net (#0)\n",
      "I0730 14:33:50.791167  5916 solver.cpp:363]     Test net output #0: accuracy = 0.395312\n",
      "I0730 14:33:50.791211  5916 solver.cpp:363]     Test net output #1: loss = 1.74054 (* 1 = 1.74054 loss)\n",
      "I0730 14:33:57.482386  5916 solver.cpp:217] Iteration 700, loss = 1.64064\n",
      "I0730 14:33:57.482434  5916 solver.cpp:234]     Train net output #0: loss = 1.64064 (* 1 = 1.64064 loss)\n",
      "I0730 14:33:57.531105  5916 solver.cpp:511] Iteration 700 (12.1527/s), lr = 0.0001\n",
      "I0730 14:33:57.767993  5916 solver.cpp:314] Iteration 704, Testing net (#0)\n",
      "I0730 14:33:58.067662  5916 solver.cpp:363]     Test net output #0: accuracy = 0.406445\n",
      "I0730 14:33:58.067698  5916 solver.cpp:363]     Test net output #1: loss = 1.68144 (* 1 = 1.68144 loss)\n",
      "I0730 14:34:05.052810  5916 solver.cpp:314] Iteration 792, Testing net (#0)\n",
      "I0730 14:34:05.352666  5916 solver.cpp:363]     Test net output #0: accuracy = 0.425391\n",
      "I0730 14:34:05.352705  5916 solver.cpp:363]     Test net output #1: loss = 1.64206 (* 1 = 1.64206 loss)\n",
      "I0730 14:34:06.018962  5916 solver.cpp:217] Iteration 800, loss = 1.67232\n",
      "I0730 14:34:06.019006  5916 solver.cpp:234]     Train net output #0: loss = 1.67232 (* 1 = 1.67232 loss)\n",
      "I0730 14:34:06.067919  5916 solver.cpp:511] Iteration 800 (11.714/s), lr = 0.0001\n",
      "I0730 14:34:12.332847  5916 solver.cpp:314] Iteration 880, Testing net (#0)\n",
      "I0730 14:34:12.632390  5916 solver.cpp:363]     Test net output #0: accuracy = 0.364648\n",
      "I0730 14:34:12.632429  5916 solver.cpp:363]     Test net output #1: loss = 1.7588 (* 1 = 1.7588 loss)\n",
      "I0730 14:34:14.249369  5916 solver.cpp:217] Iteration 900, loss = 1.62258\n",
      "I0730 14:34:14.249408  5916 solver.cpp:234]     Train net output #0: loss = 1.62258 (* 1 = 1.62258 loss)\n",
      "I0730 14:34:14.298339  5916 solver.cpp:511] Iteration 900 (12.1501/s), lr = 1e-05\n",
      "I0730 14:34:19.605340  5916 solver.cpp:314] Iteration 968, Testing net (#0)\n",
      "I0730 14:34:19.905308  5916 solver.cpp:363]     Test net output #0: accuracy = 0.438867\n",
      "I0730 14:34:19.905346  5916 solver.cpp:363]     Test net output #1: loss = 1.60653 (* 1 = 1.60653 loss)\n",
      "I0730 14:34:22.477202  5916 solver.cpp:217] Iteration 1000, loss = 1.61925\n",
      "I0730 14:34:22.477251  5916 solver.cpp:234]     Train net output #0: loss = 1.61925 (* 1 = 1.61925 loss)\n",
      "I0730 14:34:22.525977  5916 solver.cpp:511] Iteration 1000 (12.1542/s), lr = 1e-05\n",
      "I0730 14:34:26.898224  5916 solver.cpp:314] Iteration 1056, Testing net (#0)\n",
      "I0730 14:34:27.198314  5916 solver.cpp:363]     Test net output #0: accuracy = 0.439648\n",
      "I0730 14:34:27.198359  5916 solver.cpp:363]     Test net output #1: loss = 1.60188 (* 1 = 1.60188 loss)\n",
      "I0730 14:34:30.718924  5916 solver.cpp:217] Iteration 1100, loss = 1.54492\n",
      "I0730 14:34:30.718961  5916 solver.cpp:234]     Train net output #0: loss = 1.54492 (* 1 = 1.54492 loss)\n",
      "I0730 14:34:30.767911  5916 solver.cpp:511] Iteration 1100 (12.1331/s), lr = 1e-05\n",
      "I0730 14:34:34.182164  5916 solver.cpp:314] Iteration 1144, Testing net (#0)\n",
      "I0730 14:34:34.481941  5916 solver.cpp:363]     Test net output #0: accuracy = 0.442187\n",
      "I0730 14:34:34.481977  5916 solver.cpp:363]     Test net output #1: loss = 1.59486 (* 1 = 1.59486 loss)\n",
      "I0730 14:34:38.958377  5916 solver.cpp:217] Iteration 1200, loss = 1.63409\n",
      "I0730 14:34:38.958456  5916 solver.cpp:234]     Train net output #0: loss = 1.63409 (* 1 = 1.63409 loss)\n",
      "I0730 14:34:39.007135  5916 solver.cpp:511] Iteration 1200 (12.1371/s), lr = 1e-05\n",
      "I0730 14:34:41.467326  5916 solver.cpp:314] Iteration 1232, Testing net (#0)\n",
      "I0730 14:34:41.767557  5916 solver.cpp:363]     Test net output #0: accuracy = 0.442773\n",
      "I0730 14:34:41.767603  5916 solver.cpp:363]     Test net output #1: loss = 1.595 (* 1 = 1.595 loss)\n",
      "I0730 14:34:47.199411  5916 solver.cpp:217] Iteration 1300, loss = 1.64784\n",
      "I0730 14:34:47.199450  5916 solver.cpp:234]     Train net output #0: loss = 1.64784 (* 1 = 1.64784 loss)\n",
      "I0730 14:34:47.248411  5916 solver.cpp:511] Iteration 1300 (12.1341/s), lr = 1e-05\n",
      "I0730 14:34:48.753741  5916 solver.cpp:382] Snapshotting to snapshot_iter_1320.caffemodel\n",
      "I0730 14:34:48.755791  5916 solver.cpp:390] Snapshotting solver state to snapshot_iter_1320.solverstate\n",
      "I0730 14:34:48.756676  5916 solver.cpp:314] Iteration 1320, Testing net (#0)\n",
      "I0730 14:34:49.055824  5916 solver.cpp:363]     Test net output #0: accuracy = 0.444922\n",
      "I0730 14:34:49.055858  5916 solver.cpp:363]     Test net output #1: loss = 1.59124 (* 1 = 1.59124 loss)\n",
      "I0730 14:34:55.442126  5916 solver.cpp:217] Iteration 1400, loss = 1.50964\n",
      "I0730 14:34:55.442178  5916 solver.cpp:234]     Train net output #0: loss = 1.50964 (* 1 = 1.50964 loss)\n",
      "I0730 14:34:55.491098  5916 solver.cpp:511] Iteration 1400 (12.132/s), lr = 1e-05\n",
      "I0730 14:34:56.045637  5916 solver.cpp:314] Iteration 1408, Testing net (#0)\n",
      "I0730 14:34:56.346132  5916 solver.cpp:363]     Test net output #0: accuracy = 0.445898\n",
      "I0730 14:34:56.346181  5916 solver.cpp:363]     Test net output #1: loss = 1.58552 (* 1 = 1.58552 loss)\n",
      "I0730 14:35:03.331008  5916 solver.cpp:314] Iteration 1496, Testing net (#0)\n",
      "I0730 14:35:03.630942  5916 solver.cpp:363]     Test net output #0: accuracy = 0.448828\n",
      "I0730 14:35:03.630978  5916 solver.cpp:363]     Test net output #1: loss = 1.58025 (* 1 = 1.58025 loss)\n",
      "I0730 14:35:03.978365  5916 solver.cpp:217] Iteration 1500, loss = 1.56159\n",
      "I0730 14:35:03.978411  5916 solver.cpp:234]     Train net output #0: loss = 1.56159 (* 1 = 1.56159 loss)\n",
      "I0730 14:35:04.027107  5916 solver.cpp:511] Iteration 1500 (11.7151/s), lr = 1e-05\n",
      "I0730 14:35:10.613396  5916 solver.cpp:314] Iteration 1584, Testing net (#0)\n",
      "I0730 14:35:10.913312  5916 solver.cpp:363]     Test net output #0: accuracy = 0.449219\n",
      "I0730 14:35:10.913359  5916 solver.cpp:363]     Test net output #1: loss = 1.57976 (* 1 = 1.57976 loss)\n",
      "I0730 14:35:12.214139  5916 solver.cpp:217] Iteration 1600, loss = 1.62432\n",
      "I0730 14:35:12.214195  5916 solver.cpp:234]     Train net output #0: loss = 1.62432 (* 1 = 1.62432 loss)\n",
      "I0730 14:35:12.263007  5916 solver.cpp:511] Iteration 1600 (12.142/s), lr = 1e-05\n",
      "I0730 14:35:17.903606  5916 solver.cpp:314] Iteration 1672, Testing net (#0)\n",
      "I0730 14:35:18.202944  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451367\n",
      "I0730 14:35:18.202980  5916 solver.cpp:363]     Test net output #1: loss = 1.57303 (* 1 = 1.57303 loss)\n",
      "I0730 14:35:20.458839  5916 solver.cpp:217] Iteration 1700, loss = 1.63496\n",
      "I0730 14:35:20.458876  5916 solver.cpp:234]     Train net output #0: loss = 1.63496 (* 1 = 1.63496 loss)\n",
      "I0730 14:35:20.507841  5916 solver.cpp:511] Iteration 1700 (12.1289/s), lr = 1e-05\n",
      "I0730 14:35:25.188742  5916 solver.cpp:314] Iteration 1760, Testing net (#0)\n",
      "I0730 14:35:25.488673  5916 solver.cpp:363]     Test net output #0: accuracy = 0.447852\n",
      "I0730 14:35:25.488718  5916 solver.cpp:363]     Test net output #1: loss = 1.5677 (* 1 = 1.5677 loss)\n",
      "I0730 14:35:28.699123  5916 solver.cpp:217] Iteration 1800, loss = 1.48785\n",
      "I0730 14:35:28.699162  5916 solver.cpp:234]     Train net output #0: loss = 1.48785 (* 1 = 1.48785 loss)\n",
      "I0730 14:35:28.748184  5916 solver.cpp:511] Iteration 1800 (12.1354/s), lr = 1e-06\n",
      "I0730 14:35:32.480885  5916 solver.cpp:314] Iteration 1848, Testing net (#0)\n",
      "I0730 14:35:32.780539  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451367\n",
      "I0730 14:35:32.780575  5916 solver.cpp:363]     Test net output #1: loss = 1.56923 (* 1 = 1.56923 loss)\n",
      "I0730 14:35:36.943193  5916 solver.cpp:217] Iteration 1900, loss = 1.55653\n",
      "I0730 14:35:36.943241  5916 solver.cpp:234]     Train net output #0: loss = 1.55653 (* 1 = 1.55653 loss)\n",
      "I0730 14:35:36.991902  5916 solver.cpp:511] Iteration 1900 (12.1305/s), lr = 1e-06\n",
      "I0730 14:35:39.771047  5916 solver.cpp:314] Iteration 1936, Testing net (#0)\n",
      "I0730 14:35:40.070200  5916 solver.cpp:363]     Test net output #0: accuracy = 0.452539\n",
      "I0730 14:35:40.070246  5916 solver.cpp:363]     Test net output #1: loss = 1.56726 (* 1 = 1.56726 loss)\n",
      "I0730 14:35:45.181190  5916 solver.cpp:217] Iteration 2000, loss = 1.55729\n",
      "I0730 14:35:45.181316  5916 solver.cpp:234]     Train net output #0: loss = 1.55729 (* 1 = 1.55729 loss)\n",
      "I0730 14:35:45.229954  5916 solver.cpp:511] Iteration 2000 (12.1388/s), lr = 1e-06\n",
      "I0730 14:35:47.055584  5916 solver.cpp:314] Iteration 2024, Testing net (#0)\n",
      "I0730 14:35:47.355744  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451758\n",
      "I0730 14:35:47.355780  5916 solver.cpp:363]     Test net output #1: loss = 1.56746 (* 1 = 1.56746 loss)\n",
      "I0730 14:35:53.426275  5916 solver.cpp:217] Iteration 2100, loss = 1.47731\n",
      "I0730 14:35:53.426314  5916 solver.cpp:234]     Train net output #0: loss = 1.47731 (* 1 = 1.47731 loss)\n",
      "I0730 14:35:53.475014  5916 solver.cpp:511] Iteration 2100 (12.1285/s), lr = 1e-06\n",
      "I0730 14:35:54.346663  5916 solver.cpp:314] Iteration 2112, Testing net (#0)\n",
      "I0730 14:35:54.646301  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451953\n",
      "I0730 14:35:54.646333  5916 solver.cpp:363]     Test net output #1: loss = 1.56358 (* 1 = 1.56358 loss)\n",
      "I0730 14:36:01.627411  5916 solver.cpp:314] Iteration 2200, Testing net (#0)\n",
      "I0730 14:36:01.926877  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451367\n",
      "I0730 14:36:01.926913  5916 solver.cpp:363]     Test net output #1: loss = 1.56391 (* 1 = 1.56391 loss)\n",
      "I0730 14:36:01.957958  5916 solver.cpp:217] Iteration 2200, loss = 1.53759\n",
      "I0730 14:36:01.957995  5916 solver.cpp:234]     Train net output #0: loss = 1.53759 (* 1 = 1.53759 loss)\n",
      "I0730 14:36:02.006805  5916 solver.cpp:511] Iteration 2200 (11.7209/s), lr = 1e-06\n",
      "I0730 14:36:08.909205  5916 solver.cpp:314] Iteration 2288, Testing net (#0)\n",
      "I0730 14:36:09.209559  5916 solver.cpp:363]     Test net output #0: accuracy = 0.449219\n",
      "I0730 14:36:09.209605  5916 solver.cpp:363]     Test net output #1: loss = 1.56771 (* 1 = 1.56771 loss)\n",
      "I0730 14:36:10.190477  5916 solver.cpp:217] Iteration 2300, loss = 1.57395\n",
      "I0730 14:36:10.190522  5916 solver.cpp:234]     Train net output #0: loss = 1.57395 (* 1 = 1.57395 loss)\n",
      "I0730 14:36:10.239485  5916 solver.cpp:511] Iteration 2300 (12.1468/s), lr = 1e-06\n",
      "I0730 14:36:16.199398  5916 solver.cpp:314] Iteration 2376, Testing net (#0)\n",
      "I0730 14:36:16.500061  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451172\n",
      "I0730 14:36:16.500097  5916 solver.cpp:363]     Test net output #1: loss = 1.56183 (* 1 = 1.56183 loss)\n",
      "I0730 14:36:18.436863  5916 solver.cpp:217] Iteration 2400, loss = 1.47634\n",
      "I0730 14:36:18.436909  5916 solver.cpp:234]     Train net output #0: loss = 1.47634 (* 1 = 1.47634 loss)\n",
      "I0730 14:36:18.485671  5916 solver.cpp:511] Iteration 2400 (12.1269/s), lr = 1e-06\n",
      "I0730 14:36:23.494002  5916 solver.cpp:314] Iteration 2464, Testing net (#0)\n",
      "I0730 14:36:23.794368  5916 solver.cpp:363]     Test net output #0: accuracy = 0.452148\n",
      "I0730 14:36:23.794414  5916 solver.cpp:363]     Test net output #1: loss = 1.55959 (* 1 = 1.55959 loss)\n",
      "I0730 14:36:26.685735  5916 solver.cpp:217] Iteration 2500, loss = 1.47925\n",
      "I0730 14:36:26.685781  5916 solver.cpp:234]     Train net output #0: loss = 1.47925 (* 1 = 1.47925 loss)\n",
      "I0730 14:36:26.734508  5916 solver.cpp:511] Iteration 2500 (12.123/s), lr = 1e-06\n",
      "I0730 14:36:30.788022  5916 solver.cpp:314] Iteration 2552, Testing net (#0)\n",
      "I0730 14:36:31.088501  5916 solver.cpp:363]     Test net output #0: accuracy = 0.451367\n",
      "I0730 14:36:31.088547  5916 solver.cpp:363]     Test net output #1: loss = 1.56382 (* 1 = 1.56382 loss)\n",
      "I0730 14:36:34.930505  5916 solver.cpp:217] Iteration 2600, loss = 1.53775\n",
      "I0730 14:36:34.930551  5916 solver.cpp:234]     Train net output #0: loss = 1.53775 (* 1 = 1.53775 loss)\n",
      "I0730 14:36:34.979503  5916 solver.cpp:511] Iteration 2600 (12.1286/s), lr = 1e-06\n",
      "I0730 14:36:38.071528  5916 solver.cpp:382] Snapshotting to snapshot_iter_2640.caffemodel\n",
      "I0730 14:36:38.072839  5916 solver.cpp:390] Snapshotting solver state to snapshot_iter_2640.solverstate\n",
      "I0730 14:36:38.073737  5916 solver.cpp:314] Iteration 2640, Testing net (#0)\n",
      "I0730 14:36:38.374032  5916 solver.cpp:363]     Test net output #0: accuracy = 0.454883\n",
      "I0730 14:36:38.374078  5916 solver.cpp:363]     Test net output #1: loss = 1.56669 (* 1 = 1.56669 loss)\n",
      "I0730 14:36:38.374086  5916 solver.cpp:300] Optimization Done.\n",
      "I0730 14:36:38.374092  5916 caffe.cpp:178] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!/home/sandiego/Programas/caffe/build/tools/caffe train \\\n",
    "    --solver=/home/sandiego/septiembre/caffe/train_caffe/solver.prototxt \\\n",
    "    --gpu 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Entrenamiento con Digits\n",
    "\n",
    "#### Sesión interactiva\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mejorar resultados de entrenamiento con Digits\n",
    "\n",
    "<img src=\"imagenes/beagle.jpg\">\n",
    "\n",
    "---\n",
    "\n",
    "##1. Primer intento\n",
    "\n",
    "<img src=\"imagenes/red1.png\">\n",
    "\n",
    "<img src=\"imagenes/stats_red1.png\" width=50% align=left>\n",
    "<img src=\"imagenes/predict_red1.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##2. Segundo intento\n",
    "\n",
    "<img src=\"imagenes/red2.png\">\n",
    "\n",
    "<img src=\"imagenes/stats_red2.png\" width=50% align=left>\n",
    "<img src=\"imagenes/predict_red2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##3. Tercer Intento\n",
    "\n",
    "<img src=\"imagenes/red3.png\">\n",
    "\n",
    "<img src=\"imagenes/stats_red3.png\" width=50% align=left>\n",
    "<img src=\"imagenes/predict_red3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##4. Cuarto Intento\n",
    "\n",
    "<img src=\"imagenes/red4.png\">\n",
    "\n",
    "<img src=\"imagenes/stats_red4.png\" width=50% align=left>\n",
    "<img src=\"imagenes/predict_red4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##5. Quinto Intento\n",
    "\n",
    "<img src=\"imagenes/red5.png\">\n",
    "\n",
    "<img src=\"imagenes/stats_red5.png\" width=50% align=left>\n",
    "<img src=\"imagenes/predict_red5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
